---
title: "EDA, Feature Engineering & Machine Learning Fundamentals"
author: "Scott McKean"
date: "7/29/2020"
output: html_document
---

This notebook provides a very short and high level introduction to feature engineering and exploratory data analysis (EDA). The agenda for this 1 hour bootcamp includes:

1. A TED talk on EDA, features, and machine learning (15 min talk)
2. Exploratory data analysis
3. Feature engineering
4. Unsupervised learning

```{r}
library(tidyverse)
library(lubridate)
library(janitor)
library(corrplot)
library(caret)
library(fpc)
# this is the geospatial library - hard to install but incredibly useful
# in python I use geopandas
library(sf)
```

Load data

```{r}
treatment = readr::read_csv("./data/PerfTreatment_subset.csv") %>%
  janitor::clean_names()

well_header = readr::read_csv("./data/WellHeader_subset.csv") %>%
  janitor::clean_names()

production = readr::read_csv("./data/WellProduction_subset.csv") %>%
  janitor::clean_names() %>%
  mutate(date = date(production$prod_period))

prod_wide <- production %>%
  select(-x1) %>%
  select(date, ep_assets_id, prod_type, volume) %>%
  spread(key=prod_type, value=volume) %>%
  janitor::clean_names() %>%
  replace(is.na(.), 0)
```

# On EDA, Feature Engineering, and Machine Learning

This section is basically just a very short lecture on a massive subject, 
highly opinionated based on my own experience. I hope to go over what machine 
learning really is, why exploratory data analysis and feature engineering are 
so important for this competition, and then lay out the landscape of machine
learning techniques you might want to explore.

# Exploratory Data Analysis

This section is prompted by questions I might ask when approaching this dataset
in order to prime my brain for doing some feature engineering, dataset prep,
and clustering / principal component analysis.

## How many wells are we dealing with?
```{r}
# raw numbers in the production table.
prod_wide$ep_assets_id %>%
  unique() %>%
  length()

# does that match the number in the header?
well_header$ep_assets_id %>%
  unique() %>%
  length()

# but are they the same? (looks promising)
sum(unique(well_header$ep_assets_id) %in% unique(prod_wide$ep_assets_id))
```

## How long is each lateral well?
```{r}
# make surface geom
surface = st_as_sf(well_header, coords = c('surf_longitude','surf_latitude'), crs=4326) %>%
  sf::st_transform(., crs=st_crs(2956))

bottom = st_as_sf(well_header, coords = c('bh_longitude','bh_latitude'), crs=4326) %>%
  sf::st_transform(., crs=st_crs(2956))

distances = sf::st_distance(surface$geometry, bottom$geometry, by_element = TRUE)

header = well_header %>%
  mutate(lateral_dist_m = as.numeric(distances))

ggplot(surface) +
  geom_sf()

ggplot(header) +
  geom_histogram(aes(x=lateral_dist_m),bins=50)

ggplot(header) +
  geom_point(aes(x=final_drill_date, y=lateral_dist_m))
```

# What's each well's total production?
```{r}
# get the sum of all columns
prod_sums = prod_wide %>%
  dplyr::group_by(ep_assets_id) %>%
  dplyr::select(-date) %>%
  dplyr::summarise_all(., sum) %>%
  dplyr::rename_all(function(x) paste0("cum_", x)) %>%
  dplyr::rename(ep_assets_id = cum_ep_assets_id)

# gather and facet plot
prod_sums_long = prod_sums %>%
  gather(., 'category','cum',-ep_assets_id)

ggplot(prod_sums_long) +
  geom_histogram(aes(x = cum)) +
  facet_wrap(.~category, scales = 'free')

ggplot(prod_sums_long) +
  stat_ecdf(aes(x = cum)) +
  facet_wrap(.~category, scales = 'free')

# add prod_sums to header table
header = header %>%
  left_join(., prod_sums, by='ep_assets_id')

ggplot(header) +
  geom_point(aes(x=final_drill_date, y=cum_production_hours))

header %>% write_csv('./output/header.csv')
```

# Make a feature with slope of production hours vs. drill_date
```{r}
header = header %>%
  dplyr::mutate(
    prod_hours_vs_drill_date = cum_production_hours
    /as.numeric(final_drill_date-min(final_drill_date))
    )
```

# What features correlate with eachother?
```{r}
library(corrplot)

num_header = header %>%
  select(-x1) %>%
  dplyr::select_if(., is.numeric) %>%
  select(-nearZeroVar(num_header)) %>%
  drop_na()
  
jpeg(paste("./output/corrplot.jpeg",sep=""),width = 24, height = 24, units = 'in', res = 300)
corrplot(cor(num_header), tl.col = 'black', method = 'ellipse', type = 'upper', order = 'alphabet', tl.srt = 45)
dev.off()
```

# How 'dimensional' is the data?
```{r}
source('pca_functions.R')
pca_unit_circle_plot(num_header_var,'./output/')
pca_scree_plot(num_header_var,'./output/')
```

# Feature Engineering
Generally, your life is so much better if you can produce numerical 
representations of 'features' in your dataset, free of:
a) dates
b) factors / categories
c) strings
d) null values

Features can be whatever you can imagine, but my recommendations are to focus on
a) similarity/dissimilarity
b) differentiating categories/continuous variables
c) common-sense
d) crazy ideas

# Unsupervised Learning
Finally, I wanted to introduce some clustering techniques that you might want to 
consider for categorizing you data. Clustering completely depends on 'distance',
so therefore you will need numeric values (not factors or otherwise).

Since I think unsupervised learning could give the edge to competitors, I
wanted to showcase a couple unsupervised techniques below and how I would
used them as features.

```{r}
library(fpc)

# Automated k-means
kmeans = pamk(num_header)
num_header = num_header %>%
  mutate(pamk_clust = kmeans$pamobject$clustering)

# Automated density-based clustering
dbs = dbscan(num_header, eps=5000, MinPts = 6)
num_header = num_header %>%
  mutate(dbs_clust = dbs$cluster)

ggplot(num_header) +
  geom_point(aes(x=prod_hours_vs_drill_date, y=cum_oil_production_bbls, color=as.factor(dbs_clust))) +
  scale_x_log10()

ggplot(num_header) +
  geom_point(aes(x=prod_hours_vs_drill_date, y=cum_oil_production_bbls, color=as.factor(pamk_clust))) +
  scale_x_log10()
```